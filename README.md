# flinkapp-project

This project demonstrates a real-time stream processing pipeline using Apache Flink. It simulates sensor data (e.g., temperature readings) generated by a Python application and processes that data with a Flink job running in Docker containers. The processed results (average temperature per sensor) are printed to the logs, and you can monitor the system using the Flink Dashboard.

The project is organized into two main directories and a Docker Compose file:
flink-sensor-data/
├── docker-compose.yml         # Orchestrates all the Docker containers.
├── job/                       # Contains the Flink job code and build configuration.
│   ├── Dockerfile             # Builds the Flink job image.
│   ├── pom.xml                # Maven configuration to build the Flink job jar.
│   └── src/
│       └── main/
│           └── java/
│               └── com/
│                   └── example/
│                       └── SensorAverageJob.java  # The main Flink job.
└── generator/                 # Contains the sensor data generator code.
    ├── Dockerfile             # Builds the sensor generator image.
    └── sensor_server.py       # Python script to simulate sensor data.

1. docker-compose.yml
This file defines three services:

JobManager: Runs the Flink JobManager, which coordinates job execution. It uses an Apache Flink Docker image (with Java 11) and exposes port 8081 for the Flink Dashboard.
TaskManager: Runs the Flink TaskManager, which executes tasks (operators) of your Flink job. In our example, it is configured with increased memory and two task slots.
Generator: Builds and runs the sensor data generator application from the local generator/ directory. It listens on TCP port 9999 and is accessible within the Docker network by the hostname generator.

Key Points:
The JobManager and TaskManager containers use volumes to mount the built Flink job jar from the local machine.
All services are connected via a Docker network (flink-network) so that they can easily communicate (for example, the Flink job connects to the generator using its service name).

2. Job Directory
This directory holds the Flink job code and build settings.

a. pom.xml
Purpose: This Maven configuration file sets up the dependencies for Apache Flink (streaming API and client libraries) and configures the Maven Shade Plugin. The Shade Plugin creates a “fat jar” that includes all the dependencies, making it easy to deploy.
Key Settings: It specifies the Flink version, Java version (11 in this case), and designates com.example.SensorAverageJob as the main class.

b. Dockerfile (in job/)
Purpose: This multi-stage Dockerfile first builds your Java project using Maven and then creates a lightweight runtime image with OpenJDK 11.
Process:
Stage 1 (Build Stage): Uses a Maven image to compile and package the project.
Stage 2 (Runtime Stage): Copies the generated jar file (e.g., flink-sensor-data.jar) into a slim Java runtime image.

c. SensorAverageJob.java
Purpose: This is the main Flink job that defines the data processing pipeline.

Pipeline Flow:
Setup: The job creates a Flink streaming environment and forces the parallelism to 1 (to match the available TaskManager slots).
Data Source: It connects to the sensor generator by reading text data from a TCP socket on hostname generator at port 9999.
Parsing Data: The job parses each incoming line, which is expected in the format sensorId,temperature (e.g., sensor2,28.38), into a tuple containing the sensor ID, the temperature value, and a count (set to 1).
Windowing and Aggregation: Data is grouped by sensor ID using a keyBy operator. A tumbling processing time window of 5 seconds is applied to aggregate data. Within each window, a reduce function sums the temperatures and counts for each sensor.
Compute Average: After aggregation, the job maps the results to compute the average temperature per sensor (i.e., total temperature divided by count).
Output: The resulting averages are printed to standard output, which will appear in the TaskManager (or JobManager) container logs.
Execution: Finally, the job is executed.

3. Generator Directory
This directory contains the sensor data generator, which simulates live sensor readings.

a. Dockerfile (in generator/)
Purpose: Builds a Docker image using a Python 3.9 slim base image. It copies the sensor_server.py script into the container and exposes port 9999.
Behavior: The container runs the Python script as its main command.

b. sensor_server.py
Purpose:
This Python script simulates sensor data.

How It Works:
Socket Setup: The script creates a TCP socket, binds it to all interfaces (HOST set to empty string) and listens on port 9999.
Accepting a Connection: When a client (in this case, the Flink job) connects, it logs the connection.
Data Generation: Inside a loop, the script randomly selects one sensor ID from a predefined list (sensor1, sensor2, sensor3) and generates a random temperature between 20 and 30 degrees.
Sending Data: The generated sensor data is formatted as sensorId,temperature (e.g., sensor2,28.38) and sent over the socket every second.
Logging: The script prints debug messages (with flush=True to ensure immediate output) so you can see in the container logs when data is sent.

Pipeline Flow Summary
Data Generation:
The sensor generator container (running the Python script) starts up and listens on TCP port 9999. It then simulates sensor readings by sending a new message (with a sensor ID and a temperature reading) every second.

Data Ingestion:
The Flink job, when submitted, connects to the sensor generator (using the hostname generator and port 9999) and continuously reads the stream of text data.

Data Processing:
Parsing: The incoming text lines are split into sensor ID and temperature.
Aggregation: Data is grouped by sensor ID. Every 5 seconds, the job aggregates the data in that window, summing up temperatures and counts.
Calculation: The job then computes the average temperature per sensor for that window.
Output: The computed average temperatures are printed to standard output, which you can see by checking the logs of the TaskManager or JobManager containers. You can also monitor the job’s status and metrics through the Flink Dashboard on http://localhost:8081.

-------
```How to Run and Test the Project```

Build the Flink Job:
Navigate to the job/ directory.
Run: `mvn clean package -DskipTests`
* Verify that the jar (e.g., flink-sensor-data.jar) is created in the job/target folder.

Start Docker Containers:
From the project root (where docker-compose.yml is located), run: 
``docker-compose up --build -d``
* This starts the JobManager, TaskManager, and sensor generator containers.

Submit the Flink Job:
``docker exec -it flink-jobmanager flink run -d /opt/flink/usrlib/flink-sensor-data-1.0-SNAPSHOT.jar``
* Use the correct file name generated inside target folder, here `flink-sensor-data-1.0-SNAPSHOT.jar` got generated so using this in command.
* The job is submitted to the cluster, and you’ll see a job ID.

Monitoring:
Flink Dashboard: Open http://localhost:8081 in your browser to see your job, monitor its metrics, and check task statuses.

Container Logs:
For job output, check: `docker logs -f flink-taskmanager` or `docker logs -f flink-jobmanager`
For sensor generator activity, check: `docker logs -f sensor-generator`

You should see messages like:
Sensor generator: "Sent: sensor2,28.38"
Flink job: "sensor1 average temperature: 28.244999999999997" and similar lines for other sensors.

Conclusion
Sensor Data Generation: A Python script running in its own container simulates sensor data and sends it over a TCP connection.

Data Ingestion and Processing: A Flink job reads these messages from the socket, aggregates them over a fixed time window, computes average temperatures per sensor, and prints the results.
Container Orchestration: Docker Compose orchestrates all containers (JobManager, TaskManager, and Generator), ensuring they communicate over a shared network.
Monitoring: You can view job metrics and logs through the Flink Dashboard and Docker logs.



